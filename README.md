This repository contains a regularly updated paper list for **Process Supervision**.

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](./LICENSE) ![GitHub last commit (branch)](https://img.shields.io/github/last-commit/hemingkx/SpeculativeDecodingPapers/main?logo=github&color=blue)

## Training

This section focuses on the training process of reward models.

### Reward Annotation

##### Human Annotation

1. Let's Verify Step by Step. ICLR 2024. [[paper](https://aclanthology.org/2024.acl-long.510/)]
2. Solving math word problems with process- and outcome-based feedback. Arxiv 2023. [[paper](https://arxiv.org/abs/2211.14275)]
3. Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. NeurIPS 2023. [[paper](https://arxiv.org/abs/2306.01693)]

##### Automatic Annotation

1. Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning. ACL 2024. [[paper](https://aclanthology.org/2024.acl-long.510/)] 
2. Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision. EMNLP 2024 Findings. [[paper](https://arxiv.org/abs/2402.02658)] 
3. Self-Explore: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards. ArXiv 2024. [[paper](https://arxiv.org/abs/2404.10346)]
4. AutoPSV: Automated Process-Supervised Verifier. NeurIPS 2024. [[paper](https://arxiv.org/abs/2405.16802)]
5. Improve Mathematical Reasoning in Language Models by Automated Process Supervision. ArXiv 2024. [[paper](https://arxiv.org/abs/2406.06592)]
6. Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing [paper]

##### Implicit PRM (w/o need for explicit reward labels)

1. Process Reinforcement through Implicit Rewards. Blog 2024. [[blog](https://curvy-check-498.notion.site/Process-Reinforcement-through-Implicit-Rewards-15f4fcb9c42180f1b498cc9b2eaf896f)] 
2. Free Process Rewards without Process Labels. Arxiv 2024. [[paper](https://aclanthology.org/2024.acl-long.510/)]

## Reward-guided Inference

1. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Arxiv 2023. [[paper](https://arxiv.org/abs/2305.10601)]
2. Letâ€™s reward step by step: Step-level reward model as the navigators for reasoning. Arxiv 2023. [[paper](https://arxiv.org/abs/2310.10080)]
3. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. AAAI 2024. [[paper](https://arxiv.org/abs/2308.09687)]
4. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. Arxiv 2024. [[paper](http://arxiv.org/abs/2408.03314)]
5. Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers. Arxiv 2024. [[paper](https://arxiv.org/abs/2408.06195)]
6. Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning. Arxiv 2024. [[paper](https://arxiv.org/pdf/2412.15797)]
7. ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. NeurIPS 2024. [[paper](https://arxiv.org/abs/2406.03816)]
8. Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search. AAAI 2025 NeurMAD Workshop. [[paper](https://arxiv.org/abs/2501.01478)]
9. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking. Arxiv 2025.[[paper](https://arxiv.org/pdf/2501.04519)]

## Evaluation

1. ProcessBench: Identifying Process Errors in Mathematical Reasoning. Arxiv 2024. [[paper](https://arxiv.org/pdf/2412.06559)]
2. PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models. Arxiv 2024. [[paper](https://arxiv.org/pdf/2501.03124)]

## Analysis & Interpretability

1. What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning. AAAI 2025. [[paper](https://arxiv.org/pdf/2412.15904)]

## Application

1. Process Supervision-Guided Policy Optimization for Code Generation. ArXiv 2024. [[paper](https://arxiv.org/abs/2410.17621)]
2. Outcome-Refining Process Supervision for Code Generation. APlanrXiv 2024. [[paper](https://arxiv.org/abs/2412.15118)]
3. Diving into Self-Evolving Training for Multimodal Reasoning. Arxiv 2024. [[paper](https://arxiv.org/pdf/2412.17451)] [[homepage](https://mstar-lmm.github.io/)]
4. Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement. EMNLP 2024. [[paper](https://aclanthology.org/2024.emnlp-main.93.pdf)]
5. E2CL: Exploration-based Error Correction Learning for Embodied Agents. EMNLP 2024 Findings. [[paper](https://aclanthology.org/2024.findings-emnlp.448/)]
6. Process-Supervised Reward Models for Clinical Note Generation: A Scalable Approach Guided by Domain Expertise. ArXiv 2024. [[paper](https://arxiv.org/abs/2412.12583)]
7. ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark. ArXiv 2024. [[paper](https://arxiv.org/abs/2501.01290)]

## Contributing to this paper list

-  There are cases where we miss important works in this field, please feel free to contribute and promote your awesome work or other related works here! Thanks for the efforts in advance.

